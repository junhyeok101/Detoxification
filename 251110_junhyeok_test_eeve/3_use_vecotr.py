# =========================================================
# RAG í…ŒìŠ¤íŠ¸ í†µí•© ì½”ë“œ (EEVE-Korean + Vector DB)
# =========================================================

# 0) í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
import torch
import faiss
import pickle

# =========================================================
# 1) Vector DB & ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
# =========================================================
index = faiss.read_index("biased_db.index")
with open("biased_texts.pkl", "rb") as f:
    biased_texts = pickle.load(f)

embedding_model_name = "jhgan/ko-sroberta-multitask"
embedding_model = SentenceTransformer(embedding_model_name)

def retrieve(query, top_k=3):
    query_vec = embedding_model.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_vec, top_k)
    return [biased_texts[i] for i in indices[0]]

# =========================================================
# 2) LLM ë¡œë“œ (EEVE-Korean ì˜ˆì‹œ)
# =========================================================
llm_model_name = "yanolja/EEVE-Korean-Instruct-10.8B-v1.0"

print("ğŸ”¥ LLM í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë”© ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained(llm_model_name)
llm = AutoModelForCausalLM.from_pretrained(
    llm_model_name,
    device_map="auto",
    torch_dtype=torch.float16
)
print("âœ… LLM ë¡œë“œ ì™„ë£Œ")

# =========================================================
# 3) RAGìš© í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜
# =========================================================
def generate_rag_response(query, top_k=3, max_new_tokens=150):
    # 1) Vector DB ê²€ìƒ‰
    retrieved_texts = retrieve(query, top_k)
    
    # 2) ê²€ìƒ‰ ê²°ê³¼ë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
    context = "\n".join([f"- {t}" for t in retrieved_texts])
    prompt = f"ë‹¤ìŒ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:\n{context}\n\nì§ˆë¬¸: {query}\në‹µë³€:"
    
    # 3) LLM í† í¬ë‚˜ì´ì§•
    inputs = tokenizer(prompt, return_tensors="pt").to(llm.device)
    
    # 4) ë‹µë³€ ìƒì„±
    output_ids = llm.generate(**inputs, max_new_tokens=max_new_tokens)
    answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    
    return answer

# =========================================================
# 4) í…ŒìŠ¤íŠ¸
# =========================================================
query = "ìš”ì¦˜ ìŠ¤ë§ˆíŠ¸í° ê²Œì„ì´ ì™œ ë¬¸ì œì¸ê°€ìš”?"
answer = generate_rag_response(query)
print("\nâœ… RAG LLM ë‹µë³€:\n", answer)
