# ============================================
# 0) ë¼ì´ë¸ŒëŸ¬ë¦¬
# ============================================
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from datasets import Dataset
import os

# ============================================
# 1) ë°ì´í„°ì…‹
# ============================================
sft_data = [
    {"input": "ìš”ì¦˜ ê²Œì„ì€ ì „ë¶€ í˜„ì§ˆì„ ìœ ë„í•©ë‹ˆë‹¤.", 
     "output": "ìŠ¤ë§ˆíŠ¸í° ê²Œì„ì—ëŠ” ê²°ì œë¥¼ ìœ ë„í•˜ëŠ” ì‹œìŠ¤í…œì´ ìˆìŠµë‹ˆë‹¤."},
    {"input": "ì‹ ì… ê°œë°œìëŠ” ëŒ€ê¸°ì—… ê°€ì•¼ ì»¤ë¦¬ì–´ê°€ ì—´ë¦°ë‹¤.", 
     "output": "ì‹ ì… ê°œë°œìëŠ” ë‹¤ì–‘í•œ ê²½ë¡œë¡œ ì»¤ë¦¬ì–´ë¥¼ ìŒ“ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."},
]

dataset = Dataset.from_list(sft_data)

# ============================================
# 2) ëª¨ë¸ ë° í† í¬ë‚˜ì´ì €
# ============================================
model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

tokenizer.pad_token = tokenizer.eos_token
device = torch.device("cpu")
model.to(device)

def tokenize_fn(example):
    enc = tokenizer(example["input"], truncation=True, padding="max_length", max_length=32, return_tensors="pt")
    dec = tokenizer(example["output"], truncation=True, padding="max_length", max_length=32, return_tensors="pt")
    enc["labels"] = dec["input_ids"]
    return enc

tokenized_dataset = [tokenize_fn(x) for x in sft_data]

# ============================================
# 3) í•™ìŠµ ë£¨í”„ (CPU-safe)
# ============================================
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
num_epochs = 1

print("ğŸ”¥ SFT í•™ìŠµ ì‹œì‘...")
model.train()
for epoch in range(num_epochs):
    for batch in tokenized_dataset:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")

print("âœ… SFT í•™ìŠµ ì™„ë£Œ!")

# ============================================
# 4) í•™ìŠµëœ ê°€ì¤‘ì¹˜ ì €ì¥
# ============================================
os.makedirs("./sft_detox_model", exist_ok=True)
model.save_pretrained("./sft_detox_model")
tokenizer.save_pretrained("./sft_detox_model")
print("âœ… í•™ìŠµëœ ê°€ì¤‘ì¹˜ ì €ì¥ ì™„ë£Œ: ./sft_detox_model")


"""


SFT í•™ìŠµ Segmentation Fault ì›ì¸ ìš”ì•½

í™˜ê²½

CPU-only, ì»¤ë„ 5.4.x, PyTorch 2.9.0, Transformers 4.57, ë°ì´í„°ì…‹ 2ê°œ ìƒ˜í”Œ

ë¬¸ì œ ë°œìƒ

Trainer.train() ì‹¤í–‰ ì‹œ Segmentation fault

ì›ì¸ ë¶„ì„

Trainer ë‚´ë¶€ì—ì„œ DataLoader + multithreading ì‚¬ìš©

ì»¤ë„ 5.4 + MKL/OpenMP í™œì„±í™” í™˜ê²½ì—ì„œ thread spawn ì‹œ crash

ë°ì´í„°ì…‹ í¬ê¸°ì™€ GPU ì—¬ë¶€ëŠ” ì˜í–¥ ì—†ìŒ

í•´ê²° ë°©ë²•

Trainer ì—†ì´ ì§ì ‘ í•™ìŠµ ë£¨í”„ êµ¬í˜„ â†’ CPU-safe, ì •ìƒ í•™ìŠµ ì™„ë£Œ

ì¥ê¸°ì  ëŒ€ì±…: ì»¤ë„ 5.5 ì´ìƒìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ

ê²°ë¡ 

Segfault ì›ì¸: Trainer multithread + ë‚®ì€ ì»¤ë„

CPU í™˜ê²½ì—ì„œë„ ì•ˆì „í•˜ê²Œ í•™ìŠµ ê°€ëŠ¥

GPU í•„ìš” ì—†ìŒ; ì‘ì€ ëª¨ë¸/ë°ì´í„°ì…‹ì€ CPU ì¶©ë¶„

"""