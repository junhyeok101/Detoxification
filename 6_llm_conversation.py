from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

print("ğŸ”¥ ëª¨ë¸ A ë¡œë”©...")
tokenizer_a = AutoTokenizer.from_pretrained("yanolja/EEVE-Korean-Instruct-10.8B-v1.0")
model_a = AutoModelForCausalLM.from_pretrained(
    "yanolja/EEVE-Korean-Instruct-10.8B-v1.0",
    torch_dtype=torch.float16,
    device_map="auto"
)

print("ğŸ”¥ ëª¨ë¸ B ë¡œë”©...")
tokenizer_b = AutoTokenizer.from_pretrained("./sft_detox_model")
model_b = AutoModelForCausalLM.from_pretrained("./sft_detox_model")
model_b.to("cuda" if torch.cuda.is_available() else "cpu")

model_a.eval()
model_b.eval()

def generate_response_a(tokenizer, model, user_input, max_new_tokens=50):
    """ëª¨ë¸ A: 2ì¤„ ì´ë‚´ ë‹µë³€"""
    
    messages = [
        {"role": "system", "content": "ë‹¹ì‹ ì€ ê²Œì„ê³¼ í˜„ì§ˆì— ëŒ€í•´ í† ë¡  ì¤‘ì…ë‹ˆë‹¤. 2ì¤„ ì´ë‚´ì˜ ì§§ì€ ë‹µë³€ë§Œ í•˜ì„¸ìš”. ìƒˆë¡œìš´ ê´€ì ì„ ì œì‹œí•˜ì„¸ìš”."},
        {"role": "user", "content": user_input}
    ]
    
    input_text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            use_cache=False
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    if "assistant" in response:
        response = response.split("assistant")[-1].strip()
    
    # 2ì¤„ë§Œ ì¶”ì¶œ
    lines = response.split('\n')[:2]
    return '\n'.join(lines)[:100]

def generate_response_b(tokenizer, model, user_input, max_new_tokens=50):
    """ëª¨ë¸ B: 2ì¤„ ì´ë‚´ ë‹µë³€"""
    
    prompt = f"ìƒëŒ€ë°©: {user_input}\n\në‹¹ì‹ ì˜ ë‹µë³€ (2ì¤„ ì´ë‚´): "
    
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=200)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            use_cache=False
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = response.replace(prompt, "").strip()
    response = ''.join(c for c in response if ord(c) >= 0x20 or c in '\n\t')
    
    # 2ì¤„ë§Œ ì¶”ì¶œ
    lines = response.split('\n')[:2]
    return '\n'.join(lines)[:100]

print("="*70)
print("LLM ëŒ€í™”: ìš”ì¦˜ ê²Œì„ê³¼ í˜„ì§ˆ")
print("="*70)

initial_topic = "ìš”ì¦˜ ê²Œì„ì€ í˜„ì§ˆì„ ìœ ë„í•©ë‹ˆë‹¤."
conversation = []

print(f"\nğŸ¯ ì£¼ì œ: {initial_topic}\n")

for turn in range(5):
    print(f"{'='*70}")
    print(f"Turn {turn+1}")
    print(f"{'='*70}\n")
    
    # ëª¨ë¸ A
    if turn == 0:
        prompt_a = initial_topic
    else:
        # Bì˜ ë§ˆì§€ë§‰ ë‹µë³€ë§Œ ì‚¬ìš©
        prompt_a = conversation[-1]['b']
    
    response_a = generate_response_a(tokenizer_a, model_a, prompt_a)
    print(f"A: {response_a}\n")
    
    # ëª¨ë¸ B
    prompt_b = response_a
    response_b = generate_response_b(tokenizer_b, model_b, prompt_b)
    print(f"B: {response_b}\n")
    
    conversation.append({
        "turn": turn + 1,
        "a": response_a,
        "b": response_b
    })

print("="*70)
print("âœ… ëŒ€í™” ì™„ë£Œ!")
print("="*70)

# ì €ì¥
with open("conversation_log.txt", "w", encoding="utf-8") as f:
    f.write(f"ì£¼ì œ: {initial_topic}\n\n")
    for turn_data in conversation:
        f.write(f"=== Turn {turn_data['turn']} ===\n")
        f.write(f"A: {turn_data['a']}\n")
        f.write(f"B: {turn_data['b']}\n\n")

print("âœ… ë¡œê·¸ ì €ì¥: conversation_log.txt")