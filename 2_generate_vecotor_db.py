# ========================================================
# 1) ì„¤ì¹˜ (í•œ ë²ˆë§Œ)
# pip install -U sentence-transformers faiss-cpu transformers safetensors
# ========================================================

from transformers import AutoTokenizer, AutoModel
import torch
import faiss
import numpy as np
import pickle
import os

# ============================================
# 1) í¸í–¥ ë°ì´í„° (ì˜ˆì‹œ)
# ============================================
biased_texts = [
    "ìš”ì¦˜ ìŠ¤ë§ˆíŠ¸í° ê²Œì„ì€ ì „ë¶€ í˜„ì§ˆì„ ìœ ë„í•˜ëŠ” ì“°ë ˆê¸° ì‹œìŠ¤í…œì´ë¼ê³  ë³¸ë‹¤.",
    "ì–´ë–¤ ë¬¸ì œë“  ì •ë¶€ê°€ ê°œì…í•˜ë©´ ìƒí™©ì´ ë” ë‚˜ë¹ ì§„ë‹¤.",
    "ëŒ€í˜• IT ê¸°ì—…ì€ ì‚¬ìš©ì ë°ì´í„°ë¥¼ í•­ìƒ ë¶ˆë²•ì ìœ¼ë¡œ ì´ìš©í•œë‹¤.",
    "ì‹ ì… ê°œë°œìëŠ” ëŒ€ê¸°ì—…ì„ ê°€ì•¼ ì»¤ë¦¬ì–´ê°€ ì—´ë¦°ë‹¤.",
    "ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì€ íŒŒë¼ë¯¸í„°ë§Œ ëŠ˜ë¦¬ë©´ ì„±ëŠ¥ì´ ë¬´ì¡°ê±´ ì¢‹ì•„ì§„ë‹¤.",
]

# ============================================
# 2) ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë”© (safetensors)
# ============================================
model_name = "jhgan/ko-sroberta-multitask"

print("ğŸ”¥ í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë”© ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name, device_map="auto")

# ============================================
# 3) ë¬¸ì¥ â†’ ì„ë² ë”©
# ============================================
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0]
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

encoded_input = tokenizer(biased_texts, padding=True, truncation=True, return_tensors="pt")
# ì…ë ¥ì„ ëª¨ë¸ì˜ deviceë¡œ ì˜®ê¸°ê¸° (GPU or CPU)
encoded_input = {key: val.to(model.device) for key, val in encoded_input.items()}

with torch.no_grad():
    model_output = model(**encoded_input)
    sentence_embeddings = mean_pooling(model_output, encoded_input["attention_mask"])
    # GPUì—ì„œ CPUë¡œ ë³€í™˜ í›„ numpyë¡œ
    sentence_embeddings = sentence_embeddings.cpu().numpy()

print(f"ì„ë² ë”© shape: {sentence_embeddings.shape}")

# ============================================
# 4) FAISS Index ìƒì„±
# ============================================
dimension = sentence_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(sentence_embeddings)
print(f"ì´ ë²¡í„° ê°œìˆ˜: {index.ntotal}")

# ============================================
# 5) ì €ì¥
# ============================================
faiss.write_index(index, "biased_db.index")
with open("biased_texts.pkl", "wb") as f:
    pickle.dump(biased_texts, f)

print("âœ… Vector DB ì €ì¥ ì™„ë£Œ: biased_db.index")